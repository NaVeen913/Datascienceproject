{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNllBSRR8OnUs0GkmDPbdrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaVeen913/Datascienceproject/blob/main/summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "J0cH4cl8qhqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ],
      "metadata": {
        "id": "SG3bJkx9tfUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ],
      "metadata": {
        "id": "ISSkDlMWvAgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Download NLTK sentence tokenizer\n",
        "nltk.download(\"punkt\")\n"
      ],
      "metadata": {
        "id": "AwNdH_5KvtdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = (\n",
        "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
        "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
        ")\n",
        "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"])\n",
        "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ],
      "metadata": {
        "id": "6yo-jzKc1Qya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ],
      "metadata": {
        "id": "Q4Geq9wD17hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning"
      ],
      "metadata": {
        "id": "kIHEMJp39i1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "dH10_Anf9cKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sNwcGZuh9gzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet"
      ],
      "metadata": {
        "id": "0g7A_U62_YDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "os.makedirs(\"summarizer-data\", exist_ok=True)\n",
        "\n",
        "#  Load CNN/DailyMail dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# OPTIONAL: use smaller subset so it's lighter (uncomment if needed)\n",
        "# dataset[\"train\"] = dataset[\"train\"].select(range(20000))\n",
        "# dataset[\"validation\"] = dataset[\"validation\"].select(range(2000))\n",
        "# dataset[\"test\"] = dataset[\"test\"].select(range(2000))\n",
        "\n",
        "# 3) Convert to CSV and save inside summarizer-data folder\n",
        "dataset[\"train\"].to_pandas().to_csv(\"summarizer-data/train.csv\", index=False)\n",
        "dataset[\"validation\"].to_pandas().to_csv(\"summarizer-data/val.csv\", index=False)\n",
        "dataset[\"test\"].to_pandas().to_csv(\"summarizer-data/test.csv\", index=False)\n",
        "\n",
        "# 4) Zip the folder\n",
        "!zip -r summarizer-data.zip summarizer-data\n"
      ],
      "metadata": {
        "id": "M5ZpwxFpFG8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create folder for smaller dataset\n",
        "os.makedirs(\"summarizer-data\", exist_ok=True)\n",
        "\n",
        "# Load full dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "\n",
        "train_small = dataset[\"train\"].select(range(2000))        # 2000 samples\n",
        "val_small = dataset[\"validation\"].select(range(500))       # 500 samples\n",
        "test_small = dataset[\"test\"].select(range(500))            # 500 samples\n",
        "\n",
        "# Convert to CSV\n",
        "train_small.to_pandas().to_csv(\"summarizer-data/train.csv\", index=False)\n",
        "val_small.to_pandas().to_csv(\"summarizer-data/val.csv\", index=False)\n",
        "test_small.to_pandas().to_csv(\"summarizer-data/test.csv\", index=False)\n",
        "\n",
        "!zip -r summarizer-data.zip summarizer-data\n"
      ],
      "metadata": {
        "id": "2ynFu-JVFWQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download & unzip data\n",
        "!wget https://raw.githubusercontent.com/NaVeen913/Text-Summarization/main/summarizer-data.zip\n",
        "!unzip -o summarizer-data.zip\n"
      ],
      "metadata": {
        "id": "XqxqbfsEHei7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"summarizer-data/train.csv\")\n",
        "val_df = pd.read_csv(\"summarizer-data/val.csv\")\n",
        "test_df = pd.read_csv(\"summarizer-data/test.csv\")\n",
        "\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "7-EGHDWsIXYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "model_name = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "0HKGlzN2J66Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text):\n",
        "    inputs = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(model_pegasus.device)\n",
        "\n",
        "    summary_ids = model_pegasus.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=128,\n",
        "        num_beams=5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "3XRrFaMiKERS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = train_df['article'][0] if 'article' in train_df.columns else train_df.iloc[0,0]\n",
        "summary = summarize(sample_text)\n",
        "\n",
        "print(\"Original Text:\\n\", sample_text[:500], \"...\")\n",
        "print(\"\\nSummary:\\n\", summary)\n"
      ],
      "metadata": {
        "id": "rJ1KJqpFKLGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets rouge_score sentencepiece\n"
      ],
      "metadata": {
        "id": "GBFRbfmCKQdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"summarizer-data/train.csv\")\n",
        "val_df   = pd.read_csv(\"summarizer-data/val.csv\")\n",
        "test_df  = pd.read_csv(\"summarizer-data/test.csv\")\n",
        "\n",
        "print(train_df.columns)\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "loER7KvUNrHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "import torch\n",
        "\n",
        "#  Model & tokenizer\n",
        "model_name = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_pegasus = model_pegasus.to(device)\n",
        "\n",
        "# Convert pandas -> HF Dataset\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds   = Dataset.from_pandas(val_df)\n",
        "test_ds  = Dataset.from_pandas(test_df)\n",
        "\n",
        "#  Preprocessing / tokenization function\n",
        "max_input_length = 512\n",
        "max_target_length = 64\n",
        "\n",
        "def preprocess_function(batch):\n",
        "\n",
        "    inputs = batch[\"article\"]\n",
        "    targets = batch[\"highlights\"]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_input_length,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=max_target_length,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "train_tokenized = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
        "val_tokenized   = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)\n",
        "test_tokenized  = test_ds.map(preprocess_function, batched=True, remove_columns=test_ds.column_names)\n",
        "\n",
        "train_tokenized[0]\n"
      ],
      "metadata": {
        "id": "LpWFx1wzNxsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_pegasus)\n"
      ],
      "metadata": {
        "id": "fvACNpq0OMoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "batch_size = 2\n",
        "num_epochs = 2\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"pegasus-summarizer-checkpoints\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=num_epochs,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    report_to=[]\n",
        ")\n"
      ],
      "metadata": {
        "id": "dfEHFX-kOaMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets rouge_score sentencepiece -q\n"
      ],
      "metadata": {
        "id": "TeE34cUJOnTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "Mq2JXdAePJ2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch\n",
        "\n",
        "batch_size = 2\n",
        "num_epochs = 2\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    \"pegasus-summarizer-checkpoints\"\n",
        ")\n",
        "\n",
        "\n",
        "training_args.num_train_epochs = num_epochs\n",
        "training_args.per_device_train_batch_size = batch_size\n",
        "training_args.per_device_eval_batch_size = batch_size\n",
        "training_args.learning_rate = 5e-5\n",
        "training_args.weight_decay = 0.01\n",
        "training_args.logging_steps = 50\n",
        "training_args.save_total_limit = 2\n",
        "\n",
        "\n",
        "if hasattr(training_args, \"evaluation_strategy\"):\n",
        "    training_args.evaluation_strategy = \"epoch\"\n",
        "if hasattr(training_args, \"save_strategy\"):\n",
        "    training_args.save_strategy = \"epoch\"\n",
        "if hasattr(training_args, \"predict_with_generate\"):\n",
        "    training_args.predict_with_generate = True\n",
        "\n",
        "\n",
        "if hasattr(training_args, \"fp16\"):\n",
        "    training_args.fp16 = torch.cuda.is_available()\n",
        "\n"
      ],
      "metadata": {
        "id": "eDHrMFIEPQnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate rouge_score\n"
      ],
      "metadata": {
        "id": "guzw7g0FQfom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [p.strip() for p in preds]\n",
        "    labels = [l.strip() for l in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "\n",
        "\n",
        "    prediction_lens = [\n",
        "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "    ]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "\n",
        "    return {k: round(v, 4) for k in result.items()}\n"
      ],
      "metadata": {
        "id": "JZtS6jOrRfmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "B3MCgXZtWZc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ.pop(\"WANDB_DISABLED\", None)\n"
      ],
      "metadata": {
        "id": "QFTVhL1SRtY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch\n",
        "\n",
        "batch_size = 2\n",
        "num_epochs = 2\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\"pegasus-summarizer-checkpoints\")\n",
        "\n",
        "training_args.num_train_epochs = num_epochs\n",
        "training_args.per_device_train_batch_size = batch_size\n",
        "training_args.per_device_eval_batch_size = batch_size\n",
        "training_args.learning_rate = 5e-5\n",
        "training_args.weight_decay = 0.01\n",
        "training_args.logging_steps = 50\n",
        "training_args.save_total_limit = 2\n",
        "\n",
        "if hasattr(training_args, \"evaluation_strategy\"):\n",
        "    training_args.evaluation_strategy = \"epoch\"\n",
        "if hasattr(training_args, \"save_strategy\"):\n",
        "    training_args.save_strategy = \"epoch\"\n",
        "if hasattr(training_args, \"predict_with_generate\"):\n",
        "    training_args.predict_with_generate = True\n",
        "\n",
        "\n",
        "if hasattr(training_args, \"report_to\"):\n",
        "    training_args.report_to = []      # or [\"none\"]\n"
      ],
      "metadata": {
        "id": "j_SuAtcBXG40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_pegasus,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "3ikMKBm6XMpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del trainer\n"
      ],
      "metadata": {
        "id": "5jzfupAqpcqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [p.strip() for p in preds]\n",
        "    labels = [l.strip() for l in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "\n",
        "    # convert to percentage\n",
        "    final_result = {}\n",
        "    for key, value in result.items():\n",
        "        final_result[key] = round(value * 100, 4)\n",
        "\n",
        "    # add average generated length\n",
        "    prediction_lens = [\n",
        "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "    ]\n",
        "    final_result[\"gen_len\"] = round(np.mean(prediction_lens), 4)\n",
        "\n",
        "    return final_result\n"
      ],
      "metadata": {
        "id": "FkF9Ojbzr5Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_pegasus,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "K93lAKRor6Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_metrics = trainer.evaluate()\n",
        "print(val_metrics)\n"
      ],
      "metadata": {
        "id": "VU1wYgOyr9B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = trainer.predict(test_tokenized)\n",
        "print(\"Test metrics:\\n\", test_results.metrics)\n"
      ],
      "metadata": {
        "id": "YdbxobHHr-t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"pegasus-text-summarizer\"\n",
        "\n",
        "trainer.save_model(save_dir)          # saves model weights\n",
        "tokenizer.save_pretrained(save_dir)   # saves tokenizer\n",
        "\n",
        "print(\"Model saved to:\", save_dir)\n"
      ],
      "metadata": {
        "id": "jHK-66oFubnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(\"pegasus-text-summarizer\")\n",
        "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(\"pegasus-text-summarizer\").to(device)\n",
        "\n",
        "def generate_summary(text, max_len=128):\n",
        "    inputs = loaded_tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(device)\n",
        "\n",
        "    summary_ids = loaded_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=5,\n",
        "        max_length=max_len,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    return loaded_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "u2bA7uGvw1dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = train_df[\"article\"][0]\n",
        "print(\"ORIGINAL:\\n\", sample_text[:600], \"...\\n\")\n",
        "print(\"SUMMARY:\\n\", generate_summary(sample_text))\n"
      ],
      "metadata": {
        "id": "xH90bn26w5pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_yZm5xWw9HR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}